主要内容：数据挖掘中机器学习算法的优缺点以及适用数据场景

1、决策树
   决策树为分类算法，可以无压力处理特征间的交互关系并且是非参数化的，因此不必考虑异常值或者数据是否线性可分。
   优点：（1）易于理解和解释，可以可视化分析，容易提取规则；
         （2）可以同时处理标称型和数值型数据；
         （3）比较适合处理有缺失属性的样本
         （4）能够处理不相关特征
         （5）测试数据集时，运行速度比较快
         （6）在相对短的时间内能够对大型数据源做出可行且效果良好的结果
   缺点：（1）容易发生过拟合，但是随机森林可以减少过拟合
         （2）容易忽略数据集中属性的相互关联
         （3）对各类样本数量不一致的数据，进行属性划分时，不同的判定准则会带来不同的属性选择倾向；信息增益则对可取数目较多的属性有所偏好，而增益率准则CART则可对
              可取数据较少的属性有所偏好，但CART进行属性划分时不再利用增益率，而是采用一种启发式规则。        
         
 
2、C4.5
    C4.5为一种分类决策树算法，其核心是ID3算法。在继承ID3算法的优点，并且在以下几个方面对ID3算法进行了改进：
         （1）用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；
         （2）在树构造过程中进行剪枝
         （3）能够完成对连续属性的离散化处理
         （4）能够对不完整数据进行处理
    C4.5算法
    优点：
          （1）产生的分类规则易于理解，准确率高
    缺点：
          （1）在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效
    适用数据类型：数值型和标称型
    应用场景：临床决策、生产制造、文档分析、生物信息学、空间数据建模
  
  3、K-means
     K-means是聚类算法，是典型的基于距离的非层次聚类算法，在最小化误差函数的基础上将数据划分为预定的类数K，采用距离作为相似性的评价指标
     它假设对象属性来自于空间向量，并且目标是使各个群组内部的均方误差总和最小。
     优点：
          （1）容易实现，运算速度比KNN快
     缺点：
          （1）可能收敛到局部最小值，在大规模数据集上收敛较慢
          （2）聚类数目K是一个输入参数，不合适的K值可能返回较差的结果
     适用数据类型：数值型数据
     适用场景：图片分割，分析商品相似度进而归类商品，分析公司的客户分类以使用不同的商业策略
  
  4、KNN（K近邻）
     KNN是分类算法，该方法的思路：如果一个样本在特征空间中的k个最相似（即特征空间中最邻近）的样本中的大多数属于某一类别，则该样本也属于这个类别。
     优点：
         （1）精度高，无需训练
         （2）对异常值不敏感
         （3）无数据输入假定
     缺点：
         （1）计算复杂度高，空间复杂度高
         （2）可解释性较差，无法给出类似于决策树的规则
     适用数据类型：数值型和标称型
     适用场景：客户流失预测，欺诈侦测等（更适合于稀有事件的分类问题）
     
  5、SVM（支持向量机）
     SVM是一种监督式学习方法，广泛应用于统计分类以及回归分析中。核心思想：建立一个最优决策超平面，使得该平面两侧距离平面最近的两类样本之间的距离最大化从而对分类问题提供良好的泛化能力。
     平行超平面的距离或差距越大，分类器的总误差越小。
     优点：
          （1）泛化错误率低，计算开销不大
          （2）易于解释
          （3）解决非线性问题的同时避免维度灾难，可找到全局最优
      缺点：
           （1）对参数调节和核函数的选择敏感，
           （2）原始分类器不加修改仅适用于处理二类问题
           （3）运算效率低，计算时占用资源过大
       适用于数据类型：数值型和标称型数据
       适用场景：遥感图像分类，污水处理过程运行状态监控等
       
   6、Apriori
      Apriori是一种挖掘布尔关联规则频繁项集的算法。核心是基于两阶段频集思想的递推算法。该关联规则在分类上属于单维、单层、布尔关联规则。
      所有支持度大于最小支持度的项集称为频繁项集，简称频集。
      优点：
          （1）易编码实现，易理解
          （2）数据要求低
       缺点：
           （1）在大数据集上可能较慢
           （2）I/O负载过大，产生过多的候选项目集
       适用数据类型：数值型或标称型数据
       适用场景：消费市场分析，入侵检测，移动通信领域
       
    7、最大期望（EM）算法
       在统计计算中，最大期望（EM）算法是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。
       核心思想：通过E步骤和M步骤使得期望最大化
       优点：
           （1）简单稳定
       缺点：
           （1）迭代速度慢，次数多，容易陷入局部最优
       适用场景：参数估计，计算机视觉的数据集聚
       
   8、朴素贝叶斯（Naive Bayes）
      朴素贝叶斯是分类算法，其假设属性之间相互独立。在属性相关性较小时，性能最为良好。
      核心思想：通过某对象的先验概率，利用贝叶斯公式计算出其后验概率，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类
      优点：
          （1）算法简单，所需估计的参数很少，对缺失数据不太敏感
          （2）在数据较少的情况下，仍然有效，可以处理多类别问题
          （3）支持增量式运算，可以实时的对新增的样本进行训练
          （4）对大数量训练和查询时具有较高的速度。即使，使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。(适合大量数据)
          （5）易于解释
       缺点：
          （1）属性个数比较多或者属性之间相关性较大时，分类效率下降
          （2）对于输入数据的准备方式较为敏感
          （3）需要知道先验概率
          （4）分类决策存在错误率
          （5）理论上，模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的（可以考虑用聚类算法先将相关性较大的属性聚类），这给模型的正确分类带来了一定影响
      
       适用数据类型：标称型数据
       应用领域：垃圾邮件过滤，文本分类，新闻分类，Query分类，商品分类等
       
  9、AdaBoost
     AdaBoost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器 (强分类器)。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权 值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。
     优点：
        （1）泛化错误率低，易编码
        （2）可以应用在大部分分类器上，无参数调整
        （3）高精度，简单无需做特征筛选，不会过度拟合
     缺点：
        （1）对离群点敏感
        （2）训练时间过长，执行效果依赖弱分类器的选择
     适用数据类型：数值型或标称型数据
     适用场景：人脸检测，目标识别等
   
  10、GDBT（梯度提升决策树）
      GDBT分类和回归时的基学习器都是CART回归树。GBDT与AdaBoost一样可以用前向分布算法来描述。与AdaBoost的区别在于AdaBoost每次拟合基学习器时，输入的样本是不一样的
      （每一轮迭代时的样本权重不一致）。AdaBoost重点关注与上一轮分类错误的样本，GBDT是在每一步迭代时输出的值不一样，本轮拟合的输出值是之前的加法模型的预测值
      和真实值的差值（残差）。
      优点：
          （1）可以灵活处理各种类型的数据
          （2）预测的准确率高
          （3）适用一些健壮的损失函数：如huber。可以很好的处理异常值
      缺点：
         （1）由于基学习器的依赖关系，不能并行处理，不过可以通过子采样的SGBT来实现部分并行
  
  11、XgBoost
     XgBoost是许多CART回归树的集成。建立K个回归树，使得树群的预测值尽量接近真实值（准确率）而且有尽量大的泛化能力。
     XgBoost的w是最优化求出来的；正则化防止过拟合；支持并行化，在选择最佳分裂点时，进行枚举的时候并行。
     优点：
        （1）将树模型的复杂度加入到正则化中，避免过拟合
        （2）损失函数由泰勒展开式展开，同时用了一阶导和二阶导，可以加快优化速度
        （3）支持CART分类器和线性分类器，在适用线性分类器的时候还可以适用L1,L2正则化
        （4）引进了特征子采样，防止过拟合同时减少计算
        （5）在寻找最佳分类点时，实现一种近似贪心算法，用来加速和减少内存消耗，除此之外还考虑了稀疏数据集合缺失值的处理，对于特征的值有缺失样本，能够自动找到其要分裂的方向
        （6）支持特征上的并行，将特征排序后以block形式存储在内存中，在后面的迭代中重复使用此结构。在进行节点分裂时，计算每个特征的增益，最终选择增益最大的特征去做分割，那么多个特征的增益计算可以开多线程进行。
      
    适用场景：处理表格数据，

